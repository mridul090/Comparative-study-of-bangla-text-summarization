# -*- coding: utf-8 -*-
"""text_summarization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DLmxF5tWTIu4DaXMG3hJVkrm_n8cLGM6
"""

from google.colab import drive
drive.mount('/content/drive')

pip install ftfy

from nltk.corpus import stopwords
from nltk.cluster.util import cosine_distance
import numpy as np
import networkx as nx
import re
import math
import ftfy

"""# **define the regular expression for bangla text cleaning**"""

whitespace = re.compile(u"[\s\u0020\u00a0\u1680\u180e\u202f\u205f\u3000\u2000-\u200a]+", re.UNICODE)
bangla_digits = u"[\u09E6\u09E7\u09E8\u09E9\u09EA\u09EB\u09EC\u09ED\u09EE\u09EF]+"
english_chars = u"[a-zA-Z0-9]"
punc = u"[(),$%^&*+={}\[\]:\"|\'\~`<>/,¦!?½£¶¼©⅐⅑⅒⅓⅔⅕⅖⅗⅘⅙⅚⅛⅜⅝⅞⅟↉¤¿º;-]+"
bangla_fullstop = u"\u0964"     #bangla fullstop(dari)
punctSeq   = u"['\"“”‘’]+|[.?!,…]+|[:;]+"

"""# **sentence_similarity Function**"""

def sentence_similarity(sent1, sent2, stopwords):
#    if stopwords is None:
#        stopwords = []
 
    sent1 = [w.lower() for w in sent1]
    sent2 = [w.lower() for w in sent2]
    
    all_words = list(set(sent1 + sent2))
 
    vector1 = [0] * len(all_words)
    vector2 = [0] * len(all_words)
    
    
    # build the vector for the first sentence
    for w in sent1:
        if w in stopwords:
            continue
        vector1[all_words.index(w)] += 1
 
    # build the vector for the second sentence
    for w in sent2:
        if w in stopwords:
            continue
        vector2[all_words.index(w)] += 1

    return 1 - cosine_distance(vector1, vector2)

def clean_bangla_text(text):
#    text = re.sub(bangla_digits, " ", str(text))
    text = re.sub(punc, " ", str(text))
    text = re.sub(english_chars, " ", str(text))
    text = re.sub(bangla_fullstop, " ", str(text))
    text = re.sub(punctSeq, " ", str(text))
    text = whitespace.sub(" ", str(text)).strip()
    text = re.sub(r'\\','',str(text),re.UNICODE)
    text = re.sub(r'\.','',str(text),re.UNICODE)
    text = re.sub(r"\'",'',str(text),re.UNICODE)
    return text

"""# **Step 1 - Make Stop word list,Read input text and split it**"""

## below code for run in laptop
#stop_words = stopwords.words('bangla')
#print(stop_words)
#summarize_text = []    

path = '/content/drive/My Drive/Colab Notebooks/Text Summarization of my thesis/Bangla text summarization using cosine similarity/stop_words.txt'
temp_list = []
with open(path,'r',encoding = 'utf-8') as f:
  stop = f.read()
  print(stop,'\a\n\n\n\n')
  temp_list.append(stop)
  
stop_words = []
for i in temp_list:
    stop = ftfy.fix_text(i)
    stop_words.append(stop)
    print(stop)
    #print('---------------')

file = open('/content/drive/My Drive/Colab Notebooks/Text Summarization of my thesis/Bangla text summarization using cosine similarity/Document_21.txt', "r",encoding="utf8")
filedata = file.read()

#print("Input text \n")
#print(filedata)

summarize_text = []  

article = filedata.split('।')
sentences = []
#print(article

"""# **cleaning the text**"""

clean_text = []    
for data in article:
        
        cleaned_text = clean_bangla_text(data)
        clean_text.append(cleaned_text)

article = clean_text
corporas = clean_text
#print(article )
for sentence in article:
    sentences.append(sentence.split(" "))

"""# **Step 2 - Generate Similary Martix across sentences**"""

similarity_matrix = np.zeros((len(sentences), len(sentences)))

#print(similarity_matrix)

for idx1 in range(len(sentences)):
    for idx2 in range(len(sentences)):
        if idx1 == idx2: #ignore if both are same sentences
            continue 
        similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words) #find sentence similarity using the sentence_similarity function              

sentence_similarity_martix = similarity_matrix

"""# **Step 3 - Rank sentences in similarity martix**"""

sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)
scores = nx.pagerank(sentence_similarity_graph)
print(scores)

"""# **Step 4 - Sort the rank and pick top sentences**"""

ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    
#print("Indexes of top ranked_sentence order are ", ranked_sentence) 
print('\n \n ')
for score,sent in ranked_sentence:
    print(" ".join(sent),'  ',score)

summary_frequency = math.ceil(math.sqrt(len(ranked_sentence))) ##find how much summary well be generate
#print(summary_frequency)

for i in range(summary_frequency):
    summarize_text.append(" ".join(ranked_sentence[i][1]))

"""# **Step 5 - Offcourse, output the summarize texr**"""

print("\n\n\n\n\n")
print("\n Summary using cosine similarity matrix\n")
sentence = "। ".join(summarize_text)
print(sentence)
#print("। ".join(summarize_text))
#print("Real Text: \n",filedata)

dac = np.zeros(2)
for sent in corporas:
    dac[0] += 1
    
for sent in sentence.split('। '):
    dac[1] += 1
    

print('\n\n\n\n','Number of sentence in input text=',dac[0],'\n\n','Number of sentence in summary=',dac[1])

