# -*- coding: utf-8 -*-
"""Text_summarization_using_WordCount.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UYceRNTpXl5lDtOC0-7LkkNu31rWRDG3
"""

from google.colab import drive
drive.mount('/content/drive')

pip install ftfy

import bs4 as bs
import urllib.request
import re
import nltk
import heapq
from nltk.corpus import stopwords
from nltk.cluster.util import cosine_distance
import numpy as np
import networkx as nx
import re
import math
import ftfy

file = open('/content/drive/My Drive/Colab Notebooks/Text Summarization of my thesis/Bangla text summarization using cosine similarity/Document_21.txt', "r",encoding="utf-8")
filedata = file.read()

#print(filedata )
text = filedata 
# Preprocessing the data
#text = re.sub(r'\[[0-9]*\]',' ',text)
#text = re.sub(r'\s+',' ',text)
#clean_text = text.lower()

whitespace = re.compile(u"[\s\u0020\u00a0\u1680\u180e\u202f\u205f\u3000\u2000-\u200a]+", re.UNICODE)
bangla_digits = u"[\u09E6\u09E7\u09E8\u09E9\u09EA\u09EB\u09EC\u09ED\u09EE\u09EF]+"
english_chars = u"[a-zA-Z0-9]"
punc = u"[(),$%^&*+={}\[\]:\"|\'\~`<>/,¦!?½£¶¼©⅐⅑⅒⅓⅔⅕⅖⅗⅘⅙⅚⅛⅜⅝⅞⅟↉¤¿º;-]+"
punctSeq   = u"['\"“”‘’]+|[.?!,…]+|[:;]+"

text = re.sub(bangla_digits, " ", str(text))
text = re.sub(punc, " ", str(text))
text = re.sub(english_chars, " ", str(text))
text = re.sub(punctSeq, " ", str(text))
text = whitespace.sub(" ", str(text)).strip()
text = re.sub(r'\\','',str(text),re.UNICODE)
text = re.sub(r'\.','',str(text),re.UNICODE)
text = re.sub(r"\'",'',str(text),re.UNICODE)

#print('this is cleantext')

#print(text)

corporas = text

# Tokenize sentences
#sentences = nltk.sent_tokenize(text)

sentences = []
for sent in text.split('।'):
    sentences.append(sent)
#print(sentences)

# Stopword list
#below code will work in laptop
#stop_words = nltk.corpus.stopwords.words('bangla')//will be change

path = '/content/drive/My Drive/Colab Notebooks/Text Summarization of my thesis/Bangla text summarization using cosine similarity/stop_words.txt'
temp_list = []
with open(path,'r',encoding = 'utf-8') as f:
  stop = f.read()
  print(stop,'\a\n\n\n\n')
  temp_list.append(stop)
  
stop_words = []
for i in temp_list:
    stop = ftfy.fix_text(i)
    stop_words.append(stop)
    print(stop)
    #print('---------------')

# Word counts 
word2count = {}
for word in text.split(' '):
#    print(word)
    if word not in stop_words:
        if word not in word2count.keys():
            word2count[word] = 1
        else:
            word2count[word] += 1

#print('\n Word frecquency\n')
#print(word2count)

# Converting counts to weights
max_count = max(word2count.values())
for key in word2count.keys():
    word2count[key] = word2count[key]/max_count

#print('\n Word frecquency\n')
#print(word2count)
#Product sentence scores

sent2score = {}
for sentence in sentences:
    for word in sentence.split(' '):
        if word in word2count.keys():
            if len(sentence.split(' ')) < 20:
                if sentence not in sent2score.keys():
                    sent2score[sentence] = word2count[word]
                else:
                    sent2score[sentence] += word2count[word] 
         
print('\n After scoring the sentense \n')
for score_sent in sent2score.keys():
    print(score_sent,' ',sent2score[score_sent],'\n')                    
# Gettings best 5 lines             
summary_frequency = math.ceil(math.sqrt(len(sent2score))) ##find how much summary well be generate

best_sentences = heapq.nlargest(summary_frequency, sent2score, key=sent2score.get)

print('---------------------------------------------------------')
print('\n\n\n\n\n\n')
print('Summary using word count\n')
sentence = "। ".join(best_sentences)
print(sentence)

dac = np.zeros(2)
for sent in corporas.split('। '):
    dac[0] += 1

for sent in sentence.split('। '):
    dac[1] += 1


print('\n\n\n\n','Number of sentence in input text=',dac[0],'\n\n','Number of sentence in summary=',dac[1])

